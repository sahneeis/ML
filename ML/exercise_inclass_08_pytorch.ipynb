{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstEgGSzlo0F"
      },
      "source": [
        "# In-class exercise 8: PyTorch from the bottom up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hjDkYQQlo0H"
      },
      "source": [
        "Based on Jeremy Howard's PyTorch tutorial \"What is torch.nn really?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u0GMqSjlo0H"
      },
      "source": [
        "In this tutorial we will start at [PyTorch](https://pytorch.org/docs)'s lowest layer and then gradually introduce functions and features until we arrive at `nn.Sequential`. Lower layers give you more control over what you want to do, while higher layers allow for faster implementations. So in practice you have to choose at which layer you want to work. Moreover, knowing how the lower layers work will give you a better understanding of what is happening behind the scenes when working with the higher level abstractions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79BtAGkNlo0H"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import random\n",
        "from collections import OrderedDict\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make our code more clean and readable, we make use of **type annotations**. These are not necessary, but can be very helpful when working with Python and PyTorch. We also use the `typechecked` decorator to make sure that our code is type-checked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any, Callable, Dict, Optional, Set, Tuple, Union\n",
        "\n",
        "from typeguard import typechecked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A quick example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function that sums two integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = sum(1, 2)\n",
        "a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    b = sum(1, 1.2)\n",
        "except TypeError as e:\n",
        "    print(f\"TypeError: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6rWllt0lo0I"
      },
      "source": [
        "# Download the data\n",
        "In this tutorial we will be working with the **MNIST dataset**. This is a classic dataset consisting of gray-scale images of hand-drawn digits.\n",
        "\n",
        "We will use [torchvision](https://pytorch.org/vision/stable/index.html) to download the dataset. Torchvision also provides a lot of functionality for data preprocessing and augmentation, which is beyond the scope of this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x14sUfIlo0I"
      },
      "outputs": [],
      "source": [
        "# download MNIST dataset\n",
        "mnist_dev = None  # TODO\n",
        "mnist_test = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNbocEHOlo0J"
      },
      "source": [
        "MNIST consists of 70,000 28x28 images, each corresponding to a single digit (0 to 9), divided into 60,000 training images and 10,000 test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "id": "wueUC_ujlo0J",
        "outputId": "47197447-e162-40cb-9d27-9dea02f00492"
      },
      "outputs": [],
      "source": [
        "mnist_dev, mnist_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The MNIST dataset provided by torchvision is a subclass of `torch.utils.data.Dataset`. This is a PyTorch class that allows you to easily load and process data. It is a very useful class that you will use a lot when working with PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check if the dataset is an instance of torch.utils.data.Dataset\n",
        "isinstance(mnist_dev, torch.utils.data.Dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will see later how to properly use this class, but for now we will just use it to download the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The input data $\\mathbf{X}$ and targets $\\mathbf{y}$ are saved in `data` and `targets`, so we will at first just extract these and work with the raw data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_dev, y_dev = None, None  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(x_dev.shape, x_dev.dtype, x_dev.min(), x_dev.max())\n",
        "print(y_dev.shape, y_dev.dtype, y_dev.min(), y_dev.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a look at the first 10 images in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot the first 10 images\n",
        "fig, ax = plt.subplots(2, 5, figsize=(10, 4))\n",
        "for i in range(10):\n",
        "    ax[i // 5, i % 5].imshow(x_dev[i], cmap=\"gray\")\n",
        "    ax[i // 5, i % 5].set_title(f\"Label: {y_dev[i]}\")\n",
        "    ax[i // 5, i % 5].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We convert them to values between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_dev = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that setting `train=True` gives you the development set, i.e. both training and validation data. So we need to **split** this further into a training and validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# split the dataset into train and validation\n",
        "NUM_TRAIN = 50_000\n",
        "\n",
        "x_train, y_train = None, None  # TODO\n",
        "x_val, y_val = None, None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let us define the test set as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_test = None  # TODO\n",
        "y_test = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a final check to sizes, dtypes, etc. of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# check the shapes and data types\n",
        "print(\n",
        "    f\"Training data: {x_train.shape}, {y_train.shape}, {x_train.dtype}, {y_train.dtype}\"\n",
        ")\n",
        "print(f\"Validation data: {x_val.shape}, {y_val.shape}, {x_val.dtype}, {y_val.dtype}\")\n",
        "print(f\"Test data: {x_test.shape}, {y_test.shape}, {x_test.dtype}, {y_test.dtype}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We save the number of features and the number of classes in variables, so that we can use them later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define number of features and classes\n",
        "NUM_FEATURES = None  # TODO\n",
        "NUM_CLASSES = None  # TODO\n",
        "\n",
        "print(f\"Num. of features: {NUM_FEATURES}\")\n",
        "print(f\"Num. of classes: {NUM_CLASSES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reproducibility\n",
        "\n",
        "Reproducibility is important in machine learning. If you run the same code twice, you should get the same results. This is not always the case, because of the randomness involved in the training process. For example, if you initialize the weights of a neural network randomly, you will get different results each time you run the code. This is not a problem if you only run the code once, but if you want to compare different models, you need to make sure that the results are reproducible.\n",
        "\n",
        "In PyTorch, you can set the random seed using `torch.manual_seed()`. This will make sure that the results are reproducible. For custom operators, you might need to set python seed as well as the numpy random generator seed, using `random.seed()` and `np.random.seed()`.\n",
        "\n",
        "However, this is not enough. You also need to make sure that the results are reproducible on the GPU. This goes beyond the scope of this tutorial, but you can find more information [here](https://pytorch.org/docs/stable/notes/randomness.html).\n",
        "\n",
        "**TL;DR**: reproducibility is crucial, and hard to achieve (especially on the GPU). If you want to compare different models, you need to make sure that the results are reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seed():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show how seed affects the random number generation\n",
        "print(f\"Seed: 42\")\n",
        "set_seed(42)\n",
        "print(np.random.randint(0, 100, 6))\n",
        "print(np.random.randint(0, 100, 6))\n",
        "set_seed(42)\n",
        "print(f\"Seed: 42\")\n",
        "print(np.random.randint(0, 100, 3))\n",
        "print(np.random.randint(0, 100, 3))\n",
        "print(np.random.randint(0, 100, 3))\n",
        "print(np.random.randint(0, 100, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4UzMCjLlo0L"
      },
      "source": [
        "# torch.tensor\n",
        "PyTorch uses its own `torch.tensor` datatype. This is very similar to a Numpy Array, but can also be moved to and used for calculations on a GPU, and supports storing gradient information and hence dynamic backpropagation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The special function `requires_grad` allows us to specify whether we want to store gradient information for a tensor. This is useful when we want to use the tensor in a neural network, and we want to calculate the gradients of the loss function with respect to the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=False)\n",
        "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=False)\n",
        "z = torch.tensor([7.0, 8.0, 9.0], requires_grad=True)\n",
        "\n",
        "a = x + y\n",
        "b = x + z\n",
        "\n",
        "print(f\"a.requires_grad: {a.requires_grad}\")\n",
        "print(f\"b.requires_grad: {b.requires_grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Affine layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PF48Rb92lo0L"
      },
      "source": [
        "We start by manually setting up an **affine layer**. \n",
        "\n",
        "$$\n",
        "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}\n",
        "$$\n",
        "\n",
        "Let's define some weights and biases (without caring about the initialization for now). \n",
        "\n",
        "For this we use the `torch.empty` function, which creates an uninitialized tensor of the given size. (Note that `torch.empty` does not return an empty tensor, but an uninitialized tensor. This is a bit confusing, but it is what it is). Similarly one can use `torch.Tensor`, which is a special case of `torch.empty` (see [here](https://stackoverflow.com/questions/51129043/whats-the-difference-between-torch-tensor-vs-torch-empty-in-pytorch)).\n",
        "\n",
        "Note that we do not want to store gradient information for the weights and biases as of now. We will do this after we have initialized them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "weight = None  # TODO\n",
        "bias = None  # TODO\n",
        "\n",
        "weight, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now initialize the weights via **Xavier (Glorot) initialization**. Appending a function with `_` (e.g., `requires_grad_`) means that the function is applied in-place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC6HoBRSlo0L"
      },
      "outputs": [],
      "source": [
        "def sample_from_uniform():\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "\n",
        "def xavier_uniform_init():\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "\n",
        "def zeros_init():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# initialize weight using Glorot initialization and bias to zero\n",
        "weight = None  # TODO\n",
        "bias = None  # TODO\n",
        "\n",
        "weight, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISgCfOEzlo0M"
      },
      "source": [
        "The weights and biases are now initialized, and they now require gradient information.\n",
        "\n",
        "We now use these weights to create a **simple linear model** (i.e. logistic regression). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We furthermore define a loss (negative log-likelihood) for training and a function to obtain the prediction accuracy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gM8zdYBFlo0M"
      },
      "outputs": [],
      "source": [
        "def log_softmax():\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "\n",
        "def nll_loss():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now use our model to make predictions. Let's see how it performs without training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# make predictions and compute loss\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's visualize the **computational graph**. This is a very useful feature of PyTorch. It allows us to visualize the computation that is performed when we call `loss.backward()`. This is very useful when debugging your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchviz import make_dot\n",
        "from IPython.display import Image\n",
        "\n",
        "logits = model(x, weight, bias)\n",
        "loss = nll_loss(logits, label)\n",
        "params = {\"weight\": weight, \"bias\": bias}\n",
        "\n",
        "dot = make_dot(loss, params=params, show_attrs=True, show_saved=True)\n",
        "dot.render(\n",
        "    \"computational_graph\", directory=\"img\", format=\"png\"\n",
        ")  # Save the graph as a PNG file\n",
        "\n",
        "Image(\"img/computational_graph.png\", width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFOCoHtslo0N"
      },
      "source": [
        "We can now define a training loop. In this loop we need to\n",
        "1. Get a mini-batch of data. When using dynamic computation graphs like in PyTorch it is important to choose a batch size that is large enough to leverage your hardware properly.\n",
        "2. Generate predictions with our model\n",
        "3. Calculate the loss\n",
        "4. Update the gradients via `loss.backward()`\n",
        "5. Update the `weight` and `bias` based on the gradients (optimization)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xi_k9jFilo0N"
      },
      "outputs": [],
      "source": [
        "# define loss function\n",
        "# TODO\n",
        "\n",
        "# define hyperparameters\n",
        "# TODO\n",
        "\n",
        "\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# call the fit function\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MqMatYZlo0O"
      },
      "source": [
        "Nice, it works! Wasn't this already way easier than with pure Numpy? But this is just the start. Now that we've implemented our model in the lowest level of PyTorch we can start to go up the ladder and make this even better and simpler!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raGN_JN2lo0O"
      },
      "source": [
        "# torch.nn.functional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDbSMnkLlo0O"
      },
      "source": [
        "We will start by replacing some of our hand-written functions with their professionally implemented counterparts in `torch.nn.functional`. This library contains all of the PyTorch functions (other parts contain the classes). It is commonly imported via"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo4gj01blo0O"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNkEprlIlo0O"
      },
      "source": [
        "Instead of using `log_softmax` and `neg_loglikelihood` we can instead just use `F.cross_entropy`, which combines both of these.\n",
        "\n",
        "The loss should still be the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uZmWkEblo0P",
        "outputId": "346e6391-6f09-4457-e618-c9f954551ed5"
      },
      "outputs": [],
      "source": [
        "criterion = nll_loss\n",
        "pred = model(x_train[:BATCH_SIZE], weight, bias)\n",
        "target = y_train[:BATCH_SIZE]\n",
        "print(criterion(pred, target))\n",
        "\n",
        "criterion = F.cross_entropy\n",
        "pred = model(x_train[:BATCH_SIZE], weight, bias)\n",
        "target = y_train[:BATCH_SIZE]\n",
        "print(criterion(pred, target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd8c6JbIlo0P"
      },
      "source": [
        "# nn.Module\n",
        "Next we will use `nn.Module` and `nn.Parameter` for a clearer and more concise model definition and training loop. By subclassing `nn.Module` we obtain various convenience functions such as `.parameters()` and `.zero_grad()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ugm9lNslo0P"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class LogisticRegression:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNvgEMEclo0Q"
      },
      "source": [
        "Since `LogRegression` is now a class we will have to first instantiate it before using it. We can then call it as if it were a function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQvK1PgIlo0Q",
        "outputId": "60e7451c-3406-4c04-d8ed-c87116555bfb",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgBhCoIBlo0Q"
      },
      "source": [
        "We can now take advantage of `.parameters()` and `.zero_grad()` to make our training loop more concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT8JTjEBlo0Q"
      },
      "outputs": [],
      "source": [
        "# Update fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TjHdB26xlo0Q"
      },
      "outputs": [],
      "source": [
        "# call the fit function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZL9sbKxlo0R"
      },
      "source": [
        "# nn.Linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AXC2xaVlo0R"
      },
      "source": [
        "Instead of manually defining and initializing the affine layer, we can instead use the PyTorch class `nn.Linear`. PyTorch provides a wide range of predefined layers to simplify our code (and make it faster). On GitHub you will find layers for pretty much anything you might want to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCifjCWWlo0R"
      },
      "outputs": [],
      "source": [
        "# Update the model\n",
        "class LogisticRegression:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaWWz4f5lo0S"
      },
      "source": [
        "# torch.optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98W6mTVZlo0T"
      },
      "source": [
        "`torch.optim` provides various optimization algorithms. Here we will continue to use simple `SGD`, but you could just as easily switch to Adam or AMSgrad. Optimizers provide `.step()` and `.zero_grad()` methods, which allows us to make the last block in our `fit` function more concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtjcqpgllo0T"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "\n",
        "# define the optimizer\n",
        "optimizer = None  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7983Te7elo0T"
      },
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDIgAToNlo0T"
      },
      "outputs": [],
      "source": [
        "# call the fit function\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnKR9wiblo0U"
      },
      "source": [
        "# Dataset\n",
        "As already mentioned, `torch.utils.data.Dataset` is a very useful class that allows you to easily load and process data in PyTorch.\n",
        "\n",
        "A Dataset only needs to provide a `__len__` (which is called by Python's `len` function) and a `__getitem__` function for indexing the dataset.\n",
        "\n",
        "`TensorDataset` provides an easy way of converting tensors to datasets. This will make our data loading more concise, since we can handle both `x_train` and `y_train` simultaneously."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FZxIsvRlo0U"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# create Tensor datasets\n",
        "train_dataset = None  # TODO\n",
        "\n",
        "# check if the dataset is an instance of torch.utils.data.Dataset\n",
        "isinstance(train_dataset, torch.utils.data.Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xg9pJWlnlo0U"
      },
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o68EF_tolo0U",
        "outputId": "4cb2f405-89ec-4191-817d-fa5e46b10ead"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At the beginning of this tutorial we downloaded the MNIST dataset using `torchvision`. This is a subclass of `torch.utils.data.Dataset`. Instead of working with the raw data, we can instead use this class directly. This will make our code more concise and readable.\n",
        "\n",
        "Let's download the data again for demonstration purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_dev = None  # TODO\n",
        "mnist_test = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since `MNIST` is a subclass of `torch.utils.data.Dataset` it already provides `__len__` and `__getitem__` functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(len(mnist_dev))\n",
        "print(mnist_dev[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transforms\n",
        "\n",
        "The `__getitem__` method is the most important one. It is used to load and process the data. In the case of the MNIST dataset, the `__getitem__` method returns a tuple containing the image and the label of the image at the given index. \n",
        "\n",
        "As a `PIL` image is returned, we have to:\n",
        "1. convert it to a `Tensor` (we will introduce `Tensor`s later);\n",
        "2. normalize the image.\n",
        "\n",
        "Torchvision provides a `transforms` module that contains a lot of useful functions for data preprocessing and augmentation. In this case we will use the `ToTensor` and `Normalize` functions. \n",
        "\n",
        "If we want to use multiple transforms, we can use the `Compose` function to combine them into a single transform.\n",
        "\n",
        "Let's download again the MNIST dataset, but this time we will use the `transforms` module to convert the images to `Tensor`s and normalize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_dev = None  # TODO\n",
        "\n",
        "mnist_test = None  # TODO\n",
        "\n",
        "mnist_dev, mnist_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image, label = None  # TODO\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model used so far expects a vector as input, but the images are 2D. Let's implement a custom `Transform` to flatten the images.\n",
        "\n",
        "For this, we can use the `torch.view` function. This function allows us to reshape a `Tensor` without changing its data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define FlattenTransform\n",
        "class FlattenTransform:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also define a transform for the targets to convert them to `Tensor`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define IntToTensorTransform\n",
        "class IntToTensorTransform:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now download the MNIST dataset again, but this time we will use our custom `Transform` to flatten the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_dev = None  # TODO\n",
        "\n",
        "mnist_test = None  # TODO\n",
        "\n",
        "mnist_dev, mnist_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image, label = None, None  # TODO\n",
        "image.shape, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Instead of splitting the dataset into a training and validation set manually, we can use `random_split` to do this for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mnist_train, mnist_val = None, None  # TODO\n",
        "mnist_train, mnist_val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`mnist_train` and `mnist_val` are both `Subset` objects, which are subclasses of `torch.utils.data.Dataset`. They are used to select a subset of the data, and they still implement the `__getitem__` and `__len__` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image, label = None, None  # TODO\n",
        "len(mnist_train), image.shape, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's have a final check to sizes, dtypes, etc. of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\n",
        "    f\"Train | Num. of samples: {len(mnist_train)}, X shape: {mnist_train[0][0].shape}\"\n",
        ")\n",
        "print(f\"Val   | Num. of samples: {len(mnist_val)}, X shape: {mnist_val[0][0].shape}\")\n",
        "print(f\"Test  | Num. of samples: {len(mnist_test)}, X shape: {mnist_test[0][0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfrZnnr-lo0V"
      },
      "source": [
        "# DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcLI1FYIlo0V"
      },
      "source": [
        "A `DataLoader` automatically generates mini-batches for your training loop. It can run multiple workers in parallel and provides useful functionality such as data shuffling. You can create a `DataLoader` for any `Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDXxPPFslo0V"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# create DataLoader\n",
        "train_dataloader = None  # TODO\n",
        "\n",
        "# check if the dataset is an instance of torch.utils.data.DataLoader\n",
        "isinstance(train_dataloader, torch.utils.data.DataLoader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's see how the `DataLoader` works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# iterate over the DataLoader\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRDyefIClo0V"
      },
      "source": [
        "Using the DataLoader makes our training loop a lot cleaner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P3QF5u-lo0V"
      },
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gG_FoHpFlo0V",
        "outputId": "b4b2d23a-1a97-42c9-db18-0459f6f4c5a9"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NP4VoBjlo0W"
      },
      "source": [
        "# Validation\n",
        "\n",
        "Now that we have a training loop we can go ahead and do some real work. To avoid overfitting, enable early stopping and have some information for model development we always need a validation set.\n",
        "\n",
        "Since the validation set does not need backpropagation we can use 2x larger batches for it. Furthermore, we should shuffle our training data to avoid correlation between batches. This is not necessary (and would waste computation time) for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nq5V_fdlo0W"
      },
      "outputs": [],
      "source": [
        "dataloaders = {}\n",
        "dataloaders[\"train\"] = None  # TODO\n",
        "dataloaders[\"val\"] = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maBDs2ablo0W"
      },
      "source": [
        "Note that you need to call `model.train()` before training and `model.eval()` before evaluation (inference), since some layers like dropout and batch normalization work differently in each mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rfMLPlflo0W"
      },
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO6SonHYlo0W",
        "outputId": "b4ad4edd-9ae5-436c-b7e9-770560f0ad16"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What happens if we let the model train for more epochs?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(\"img/validation_curves_shuffle.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Image(\"img/validation_curves_no_shuffle.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we continue, let's make our training loop a bit more concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define run_epoch function\n",
        "def run_epoch():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing\n",
        "\n",
        "Now that we have a trained model we can use it to make predictions on the test set. We can then use these predictions to calculate the accuracy on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_dataloader = None  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = run_epoch()  # TODO\n",
        "print(f\"Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k0GNIYGlo0X"
      },
      "source": [
        "# CNN\n",
        "Using simple logistic regression (or an MLP) for images basically ignores the data's underlying structure. We can do much better than this by switching to a CNN. Since our training loop does not assume anything about the model we can train a CNN without any changes.\n",
        "\n",
        "Our CNN will consist of 3 convolutional layers, each using PyTorch's predefined `Conv2d` layer. At the End, we perform average pooling. Since `Conv2d` assumes a shape of `[batch_size, num_channels, height, width]` we need to reshape our input inside the model via `.view(_)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfi6evjJlo0X"
      },
      "outputs": [],
      "source": [
        "# Define the CNN model\n",
        "class CNN:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XOOcNyclo0X"
      },
      "source": [
        "We will now furthermore use momentum in our optimizer to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Afl2sExolo0X",
        "outputId": "e01cb93e-f1cc-4fd4-88d0-c26c966684db"
      },
      "outputs": [],
      "source": [
        "NUM_CHANNELS = 16\n",
        "LEARNING_RATE = 0.1\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_epoch()  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-lPL9n4lo0Z"
      },
      "source": [
        "# GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8iFid0lo0Z"
      },
      "source": [
        "PyTorch can run significantly faster on a GPU than on a CPU, so you should always try to leverage that hardware. To do so, you need to move both your model and your data to the device.\n",
        "\n",
        "So let's first check if you have a GPU and choose the appropriate device.\n",
        "\n",
        "**Note**: If you are using Google Colab, you can go to `Runtime -> Change runtime type` and select `GPU` as hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_57uBFolo0Z",
        "outputId": "02b7c9c7-e43f-418c-e858-05e9e580275b"
      },
      "outputs": [],
      "source": [
        "# check if GPU is available\n",
        "# TODO\n",
        "\n",
        "# set the device to GPU if available\n",
        "device = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf7Frmaflo0Z"
      },
      "source": [
        "Now, we can move our model to the device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEajfTSllo0Z"
      },
      "outputs": [],
      "source": [
        "# check model device\n",
        "# TODO\n",
        "\n",
        "# move the model to the device\n",
        "# TODO\n",
        "\n",
        "# check model device\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da2LmBpQlo0a"
      },
      "source": [
        "Then, we redefine the DataLoader to:\n",
        "1. **pin the memory**: this is a trick that will accelerate moving data between CPU and GPU (have a look at [the documentation](https://pytorch.org/docs/stable/data.html#memory-pinning) and [this explanation](https://stackoverflow.com/a/55564072) for more information);\n",
        "2. **use more workers**: the `num_workers` argument specifies how many subprocesses to use for data loading. Setting it to a higher number will speed up data loading (have a look at [the documentation](https://pytorch.org/docs/stable/data.html#multi-process-data-loading) for more information).\n",
        "\n",
        "**Note**: Jupyter notebooks do not play well with multiprocessing, so you might want to set `num_workers=0` if you are using a Jupyter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaFMWQqflo0a"
      },
      "outputs": [],
      "source": [
        "NUM_WORKERS = 0\n",
        "\n",
        "dataloaders = {}\n",
        "dataloaders[\"train\"] = None  # TODO\n",
        "dataloaders[\"val\"] = None  # TODO\n",
        "dataloaders[\"test\"] = None  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the instantiate_dataloaders function\n",
        "def instantiate_dataloaders():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### How to set the number of workers? \n",
        "\n",
        "Well, it depends on your hardware. The first thing you should do is to check how many cores your CPU has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "\n",
        "num_cores = multiprocessing.cpu_count()\n",
        "print(\"Number of CPU cores:\", num_cores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secondly, you should do some quick benchmarking to see how the number of workers affects the data loading time. You can do this by setting `num_workers` to different values and measuring the time it takes to load the data. You can use the `%%timeit` magic command to do this. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%timeit -n 1 -r 3\n",
        "for x_batch, y_batch in dataloaders[\"train\"]:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BIsLw2Zlo0a"
      },
      "source": [
        "Finally, we need to slightly change our training loop to send each batch to the device first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rA7dwcbHlo0a"
      },
      "outputs": [],
      "source": [
        "# Update the run_epoch function\n",
        "def run_epoch():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdbOMYylo0a"
      },
      "source": [
        "And now we can run our CNN on the GPU!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSXfu4MAlo0a",
        "outputId": "0e864546-2b37-4f08-96e8-0cd4a3e274f3"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "dataloaders = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = run_epoch()  # TODO\n",
        "print(f\"Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRz3UH8xlo0X"
      },
      "source": [
        "# nn.Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mx_G4xTlo0Y"
      },
      "source": [
        "PyTorch provides a class `nn.Sequential` for simplifying the definition of modules that only consist of a stack of layers. Since these are exactly the models we have been using so far we will now switch to this interface.\n",
        "\n",
        "Because not all functions are defined as PyTorch layers we will start by defining a module that just converts a function to a layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39IzIimqlo0Y"
      },
      "outputs": [],
      "source": [
        "# Define module that converts a function into a layer\n",
        "class Lambda:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgBce3M5lo0Y"
      },
      "source": [
        "We can now define our CNN in a more concise manner. Note that we now use `nn.AdaptiveAvgPool2d`, which allows us to specify the size of the output tensor instead of the input tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nlsu9b8zlo0Y"
      },
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "model = None  # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we don't have control on the model class (i.e., we cannot define a member function such as `init_weights` within the class itself), we can define a function and apply it to the model with `.apply(_)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define initialize_weights function\n",
        "def initialize_weights():\n",
        "    # TODO\n",
        "    pass\n",
        "\n",
        "\n",
        "# Apply initialize_weights to the model\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1fre6h-lo0Y",
        "outputId": "fc0d2350-075f-4b73-c106-df4ebcbd95f1"
      },
      "outputs": [],
      "source": [
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "dataloaders = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = run_epoch()  # TODO\n",
        "print(f\"Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# nn.ModuleList"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A similar approach to `nn.Sequential` is `nn.ModuleList`, which allows you to define a list of layers. This is useful when you want to define a model that has a variable number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the model\n",
        "class CNN:\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "NUM_LAYERS = 3\n",
        "model = None  # TODO\n",
        "criterion = F.cross_entropy\n",
        "optimizer = None  # TODO\n",
        "dataloaders = None  # TODO\n",
        "\n",
        "fit()  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_loss, test_acc = run_epoch()  # TODO\n",
        "print(f\"Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Early stopping and checkpointing\n",
        "\n",
        "Early stopping is a very useful technique for avoiding overfitting. It works by monitoring the validation loss and stopping training when it starts to increase. This is a very simple and effective technique that you should always use.\n",
        "\n",
        "We can implement early stopping by keeping track of the best model and stopping training when the validation loss starts to increase.\n",
        "\n",
        "We can now use this to implement checkpointing. This is a technique for saving the model at regular intervals during training. This allows us to load the model from a previous checkpoint if something goes wrong during training. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### State dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(model.state_dict()), len(\n",
        "    model.state_dict()\n",
        "), model.state_dict().keys(), model.state_dict()[\"layers.0.weight\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the fit function\n",
        "def fit():\n",
        "    # TODO\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(SEED)\n",
        "PATIENCE = 3\n",
        "model = None  # TODO\n",
        "optimizer = None  # TODO\n",
        "dataloaders = None  # TODO\n",
        "\n",
        "best_model_state_dict = fit()  # TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load the model from state dict\n",
        "# TODO\n",
        "\n",
        "test_loss, test_acc = run_epoch()  # TODO\n",
        "print(f\"Test - loss: {test_loss:.4f}, acc: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRKt29d5lo0a"
      },
      "source": [
        "# Summary\n",
        "Great, so now we have a general training loop and know how to quickly define new models! Now let us sum up what we have learned during this journey:\n",
        "\n",
        "\n",
        "**Reproducibility**: To make sure that your results are reproducible, you need to set the random seed using `torch.manual_seed()`. You might also need to set the python seed and the numpy random generator seed. Additional attention is needed when using the GPU (beyond the scope of this tutorial).\n",
        "\n",
        "**Type annotations**: Type annotations are not necessary, but can be very helpful when working with Python and PyTorch. We also use the `typechecked` decorator to make sure that our code is type-checked (meaning that the types are checked at runtime).\n",
        "\n",
        "`torch.tensor`: PyTorch tensors work like Numpy arrays, but can remember gradients and be sent to the GPU.\n",
        "\n",
        "`torch.nn`: Many useful layers are already implemented in this library, e.g. `nn.Linear` or `nn.Conv2d`.\n",
        "   \n",
        "- `torch.nn.functional`: Provides various useful functions (non stateful) for training neural networks, e.g. activation and loss functions.\n",
        "- `torch.nn.Module`: Subclass from this to create a callable that acts like a function, but can remember state. It knows what `Parameter`s and submodules it contains and provides various functionality based on that.\n",
        "- `torch.nn.Parameter`: Wraps a tensor and tells the containing Module that it needs updating during backpropagation.\n",
        "- `torch.nn.Sequential`: Provides an easy way of defining purely stacked modules.\n",
        "- `torch.nn.ModuleList`: Provides an easy way of defining a list of modules.\n",
        "\n",
        "\n",
        "`torch.optim`: Optimizers such as `SGD` or `Adam`, which let you easily update and train the `Parameter`s inside the passed model.\n",
        "\n",
        "`torch.utils.data.Dataset`: An abstract class for representing a dataset. Provides an easy way of loading and processing data. Any dataset should subclass this class and implement `__len__` and `__getitem__`. Tensors can be converted to datasets using `TensorDataset`.\n",
        "\n",
        "`torch.utils.data.DataLoader`: Takes any `Dataset` and provides an iterator for returning mini-batches with various advanced functionality.\n",
        "\n",
        "**GPU**: To use your GPU you need to move your model and each mini-batch to your GPU using `.to(device)`.\n",
        "\n",
        "**Checkpointing**: To save and load your model's state use `torch.save` and `torch.load`.\n",
        "\n",
        "Other useful libraries:\n",
        "- `torchvision`: Provides various useful datasets, transforms and models for computer vision.\n",
        "- `torchviz`: Provides visualization of PyTorch computational graphs.\n",
        "\n",
        "\n",
        "Optional:\n",
        "- `torchtyping`: Provides type annotations for PyTorch tensors, including shape and dtype information."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
