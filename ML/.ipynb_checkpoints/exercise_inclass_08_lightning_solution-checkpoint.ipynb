{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise 8: Converting your PyTorch code to PyTorch Lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on [Lightning in 15 minutes](https://lightning.ai/docs/pytorch/stable/starter/introduction.html) tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will convert the code from the previous tutorial to PyTorch Lightning. This will allow us to reduce the amount of boilerplate code we need to write, and also make it easier to train our model on multiple GPUs or even TPUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch Lightning is a lightweight **wrapper** for **organizing** your PyTorch code. It's **not a high-level framework**, so you still have to write PyTorch code, but it handles a lot of the details for you. It's especially useful for **standardizing** training loops, logging metrics, and saving checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we install PyTorch Lightning and import the relevant classes and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import OrderedDict\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from typeguard import typechecked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we report here some code from the previous tutorial for reference purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: we re-define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "class CNN(nn.Module):\n",
    "    \"\"\"Convolutional neural network model.\"\"\"\n",
    "\n",
    "    def __init__(self, num_layers: int, num_channels: int, num_classes: int) -> None:\n",
    "        \"\"\"Constructor method for CNN.\n",
    "\n",
    "        Args:\n",
    "            num_layers: the number of layers\n",
    "            num_channels: the number of channels\n",
    "            num_classes: the number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = 1 if i == 0 else num_channels\n",
    "            out_channels = num_channels if i < num_layers - 1 else num_classes\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize the parameters.\n",
    "\n",
    "        The weight is initialized using Xavier uniform initialization and the bias is initialized to zero.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "\n",
    "        Returns:\n",
    "            the logits\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we re-define the dataset and dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(\n",
    "    data_dir: str, train_size: int\n",
    ") -> Tuple[\n",
    "    torch.utils.data.Dataset, torch.utils.data.Dataset, torch.utils.data.Dataset\n",
    "]:\n",
    "    \"\"\"Get the datasets.\n",
    "\n",
    "    Args:\n",
    "        data_dir: the directory to store the data\n",
    "        train_size: the size of the training set\n",
    "\n",
    "    Returns:\n",
    "        the training set, validation set, and test set\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    "    dev_set = MNIST(data_dir, train=True, download=True, transform=transform)\n",
    "    train_set, val_set = random_split(dev_set, [train_size, len(dev_set) - train_size])\n",
    "    test_set = MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "    return train_set, val_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_dataloaders(\n",
    "    train_dataset: torch.utils.data.Dataset,\n",
    "    val_dataset: torch.utils.data.Dataset,\n",
    "    test_dataset: torch.utils.data.Dataset,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    seed: int,\n",
    ") -> Dict[str, torch.utils.data.DataLoader]:\n",
    "    \"\"\"Instantiate dataloaders.\n",
    "\n",
    "    Args:\n",
    "        train_dataset: the training dataset\n",
    "        val_dataset: the validation dataset\n",
    "        test_dataset: the test dataset\n",
    "        batch_size: the batch size\n",
    "        num_workers: the number of workers\n",
    "        seed: the seed\n",
    "\n",
    "    Returns:\n",
    "        the dictionary of dataloaders\n",
    "    \"\"\"\n",
    "    dataloaders = {}\n",
    "    dataloaders[\"train\"] = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        generator=torch.Generator().manual_seed(seed),\n",
    "    )\n",
    "    dataloaders[\"val\"] = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=2 * batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        generator=torch.Generator().manual_seed(seed),\n",
    "    )\n",
    "    dataloaders[\"test\"] = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=2 * batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=num_workers,\n",
    "        generator=torch.Generator().manual_seed(seed),\n",
    "    )\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if everything works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(32, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      ")\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "model = CNN(num_layers=3, num_channels=32, num_classes=10)\n",
    "print(model)\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "    data_dir=\"data\", train_size=50000\n",
    ")\n",
    "dataloaders = instantiate_dataloaders(\n",
    "    train_dataset=train_dataset,\n",
    "    val_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "print(next(iter(dataloaders[\"train\"]))[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's finally re-define the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@typechecked\n",
    "def run_epoch(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    criterion: Callable,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    optimizer: Optional[torch.optim.Optimizer] = None,\n",
    "    train: bool = False,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"Run one epoch.\n",
    "\n",
    "    It runs one epoch of training, validation, or test, and returns the loss and accuracy. If training is True, it also updates the parameters.\n",
    "\n",
    "    Args:\n",
    "        model: the model (an instance of torch.nn.Module)\n",
    "        dataloader: the dataloader (an instance of torch.utils.data.DataLoader)\n",
    "        criterion: a callable that returns the loss given the logits and the labels\n",
    "        device: the device (cpu or gpu). Defaults to torch.device(\"cpu\").\n",
    "        optimizer: the optimizer (an instance of torch.optim.Optimizer). Defaults to None.\n",
    "        train: whether to train the model. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        the loss and accuracy\n",
    "    \"\"\"\n",
    "    epoch_loss = 0.0\n",
    "    epoch_acc = 0.0\n",
    "\n",
    "    # Move model to the device\n",
    "    model = model.to(device)\n",
    "\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Move data to the device\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        with torch.set_grad_enabled(train):\n",
    "            pred = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "\n",
    "            if train:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += (pred.argmax(-1) == y_batch).sum().item()\n",
    "\n",
    "    epoch_loss /= len(dataloader.dataset)\n",
    "    epoch_acc /= len(dataloader.dataset)\n",
    "\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def fit(\n",
    "    model: torch.nn.Module,\n",
    "    dataloaders: Dict[str, torch.utils.data.DataLoader],\n",
    "    criterion: Callable,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    num_epochs: int,\n",
    "    patience: int,\n",
    "    device: torch.device,\n",
    ") -> OrderedDict[str, torch.Tensor]:\n",
    "    \"\"\"Train the model.\n",
    "\n",
    "    Args:\n",
    "        model: the model (an instance of torch.nn.Module)\n",
    "        dataloaders: the dictionary of dataloaders (an instance of torch.utils.data.DataLoader)\n",
    "        criterion: a callable that returns the loss given the logits and the labels\n",
    "        optimizer: the optimizer (an instance of torch.optim.Optimizer)\n",
    "        num_epochs: the number of epochs\n",
    "        patience: the patience for early stopping\n",
    "        device: the device (cpu or gpu)\n",
    "\n",
    "    Returns:\n",
    "        the state dict of the best model\n",
    "    \"\"\"\n",
    "    loss_history = {\"train\": [], \"val\": []}\n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    curr_patience = patience\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_acc = run_epoch(\n",
    "            model, dataloaders[\"train\"], criterion, device, optimizer, train=True\n",
    "        )\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_acc = run_epoch(\n",
    "            model, dataloaders[\"val\"], criterion, device, train=False\n",
    "        )\n",
    "\n",
    "        loss = {\"train\": train_loss, \"val\": val_loss}\n",
    "        acc = {\"train\": train_acc, \"val\": val_acc}\n",
    "        print_epoch_summary(epoch, num_epochs, loss, acc)\n",
    "\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        acc_history[\"train\"].append(train_acc)\n",
    "        acc_history[\"val\"].append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            curr_patience = patience\n",
    "            # Save the best model state dict as a checkpoint\n",
    "            ckpt = copy.deepcopy(model.state_dict())\n",
    "            # Save the best model to disk\n",
    "            torch.save(ckpt, \"ckpt.pt\")\n",
    "        else:\n",
    "            curr_patience -= 1\n",
    "            if curr_patience == 0:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "    training_history = {\"loss\": loss_history[\"train\"], \"acc\": acc_history[\"train\"]}\n",
    "    validation_history = {\"loss\": loss_history[\"val\"], \"acc\": acc_history[\"val\"]}\n",
    "    plot_curves(training_history, validation_history)\n",
    "\n",
    "    return ckpt\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def print_epoch_summary(\n",
    "    epoch: int, num_epochs: int, loss: Dict[str, float], acc: Dict[str, float]\n",
    ") -> None:\n",
    "    \"\"\"Print the epoch summary.\n",
    "\n",
    "    The summary includes the epoch number, the number of epochs, the loss, and the accuracy.\n",
    "\n",
    "    Args:\n",
    "        epoch: the epoch number\n",
    "        num_epochs: the number of epochs\n",
    "        loss: the loss\n",
    "        acc: the accuracy\n",
    "    \"\"\"\n",
    "    print(\n",
    "        f\"Epoch {epoch+1:>{len(str(num_epochs))}}/{num_epochs} | \"\n",
    "        f\"Train - loss: {loss['train']:.4f}, acc: {acc['train']:.4f} | \"\n",
    "        f\"Val - loss: {loss['val']:.4f}, acc: {acc['val']:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "@typechecked\n",
    "def plot_curves(\n",
    "    training_history: Dict[str, list], validation_history: Dict[str, list]\n",
    ") -> None:\n",
    "    \"\"\"Plot the loss and accuracy curves.\n",
    "\n",
    "    It plots the loss curve on the left and the accuracy curve on the right via matplotlib.\n",
    "\n",
    "    Args:\n",
    "        training_history: the training history\n",
    "        validation_history: the validation history\n",
    "    \"\"\"\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    ax1.plot(training_history[\"loss\"], label=\"Training\")\n",
    "    ax1.plot(validation_history[\"loss\"], label=\"Validation\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax2.plot(training_history[\"acc\"], label=\"Training\")\n",
    "    ax2.plot(validation_history[\"acc\"], label=\"Validation\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax2.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 3\n",
    "\n",
    "# data\n",
    "NUM_WORKERS = 3\n",
    "\n",
    "# optimizer\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# model\n",
    "NUM_LAYERS = 3\n",
    "NUM_CHANNELS = 32\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: replace `nn.Module` with `LightningModule`\n",
    "\n",
    "- 1.1: Model architecture goes in the `__init__` method\n",
    "- 1.2: Prediction/inference logic goes in the `forward` hook\n",
    "- 1.3: Optimizers go in the `configure_optimizers` hook\n",
    "- 1.4: Training logic goes in the `training_step` hook\n",
    "- 1.5: Validation logic goes in the `validation_step` hook\n",
    "- 1.6: Test logic goes in the `test_step` hook\n",
    "- 1.7: Remove any `cuda()` or `to(device)` calls\n",
    "- 1.8: Instantiate the `LightningModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLit(pl.LightningModule):\n",
    "    \"\"\"Convolutional neural network model in PyTorch Lightning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        num_channels: int,\n",
    "        num_classes: int,\n",
    "        optimizer_args: Dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor method for CNNLit.\n",
    "\n",
    "        Args:\n",
    "            num_layers: the number of layers\n",
    "            num_channels: the number of channels\n",
    "            num_classes: the number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Step 1.1: Define the model architecture\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = 1 if i == 0 else num_channels\n",
    "            out_channels = num_channels if i < num_layers - 1 else num_classes\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.optimizer_args = optimizer_args\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize the parameters.\n",
    "\n",
    "        The weight is initialized using Xavier uniform initialization and the bias is initialized to zero.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "\n",
    "        Returns:\n",
    "            the logits\n",
    "        \"\"\"\n",
    "        # Step 1.2: Define the forward pass (inference/prediction logic)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            the optimizer\n",
    "        \"\"\"\n",
    "        # Step 1.3: Define the optimizer\n",
    "        optimizer = torch.optim.SGD(self.parameters(), **self.optimizer_args)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "\n",
    "        Returns:\n",
    "            the loss\n",
    "        \"\"\"\n",
    "        # Step 1.4: Define the training logic\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Validation step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        # Step 1.5: Define the validation logic\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Test step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        # Step 1.6: Define the test logic\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "\n",
    "# Step 1.7: Remember to remove any call to `cuda()` or `to(device)` in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1.8: Create an instance of the `LitModule` class\n",
    "lit_module = CNNLit(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    optimizer_args={\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"momentum\": MOMENTUM,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNLit(\n",
       "  (layers): ModuleList(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(32, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): AdaptiveAvgPool2d(output_size=1)\n",
       "  )\n",
       "  (criterion): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit_module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: replace the training loop with a `Trainer` instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `LightningModule` is defined, we can train it using a `Trainer`.\n",
    "\n",
    "- 1: Instantiate the `Trainer`\n",
    "- 2: Call `trainer.fit(model, train_dataloader, val_dataloader)` to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/filippoguerranti/.pyenv/versions/3.10.4/envs/cbsnn/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | layers    | ModuleList       | 12.5 K\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a37eb7f3ac149478ae8c9fe4457ed7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filippoguerranti/.pyenv/versions/3.10.4/envs/cbsnn/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/filippoguerranti/.pyenv/versions/3.10.4/envs/cbsnn/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842924cf29f54edba0f6a42f3f0302ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21946ecdc158424c817cacb67a78ae6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "trainer.fit(lit_module, dataloaders[\"train\"], dataloaders[\"val\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filippoguerranti/.pyenv/versions/3.10.4/envs/cbsnn/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8858f4704ce34f20883af9ae97348726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6059901118278503     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6059901118278503    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6059901118278503}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lit_module, dataloaders[\"test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: replace the dataset and dataloaders with `LightningDataModule`\n",
    "\n",
    "- 1: Move the dataset and dataloaders into a `LightningDataModule`\n",
    "- 2: Instantiate the `LightningDataModule`\n",
    "- 3: Pass the `LightningDataModule` to the `Trainer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `LightningDataModule` encapsulates the five steps involved in data processing in PyTorch:\n",
    "- 2.1: Download / tokenize / process.\n",
    "- 2.2: Clean and (maybe) save to disk.\n",
    "- 2.3: Load inside Dataset.\n",
    "- 2.4: Apply transforms (rotate, tokenize, etc…).\n",
    "- 2.5: Wrap inside a DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = \"./data\",\n",
    "        batch_size: int = 32,\n",
    "        train_size: int = 50000,\n",
    "        num_workers: int = 0,\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor method for MNISTDataModule.\n",
    "\n",
    "        Args:\n",
    "            data_dir: the directory to store the data. Defaults to \"./data\".\n",
    "            batch_size: the batch size. Defaults to 32.\n",
    "            train_size: the size of the training set. Defaults to 50000.\n",
    "            num_workers: the number of workers. Defaults to 0.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.train_size = train_size\n",
    "        self.val_size = 60_000 - train_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.transform = torchvision.transforms.Compose(\n",
    "            [\n",
    "                torchvision.transforms.ToTensor(),\n",
    "                torchvision.transforms.Normalize((0.1307,), (0.3081,)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        # Download\n",
    "        torchvision.datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        torchvision.datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_dev = torchvision.datasets.MNIST(\n",
    "                self.data_dir, train=True, transform=self.transform\n",
    "            )\n",
    "            self.mnist_train, self.mnist_val = torch.utils.data.random_split(\n",
    "                mnist_dev,\n",
    "                [self.train_size, self.val_size],\n",
    "                generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = torchvision.datasets.MNIST(\n",
    "                self.data_dir, train=False, transform=self.transform\n",
    "            )\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            # The difference btw \"predict\" and \"test\" is that the former does not have labels\n",
    "            pass  # We will not use this in this exercise\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_train,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_val,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.mnist_test,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            generator=torch.Generator().manual_seed(42),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = MNISTDataModule(\n",
    "    data_dir=\"data\", batch_size=BATCH_SIZE, train_size=50000, num_workers=NUM_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNNLit(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    optimizer_args={\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"momentum\": MOMENTUM,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/filippoguerranti/.pyenv/versions/3.10.4/envs/cbsnn/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | layers    | ModuleList       | 12.5 K\n",
      "1 | criterion | CrossEntropyLoss | 0     \n",
      "-----------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e074e827960c456580a75847ad113d12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71be8b238de64e099409fde1de4cdd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99cdc8a498c4728be1a5b12c79e4373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe802668e9fa43e08c7f5f74a950bc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.6921523213386536     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6921523213386536    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.6921523213386536}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we were only able to log the loss. However, Lightning allows us to have much more control over logging. For example, we can log the loss and accuracy after each epoch, and also log the loss and accuracy after each batch. We can even log images, audio, text, and arbitrary objects.\n",
    "\n",
    "We will be using [TorchMetrics](https://torchmetrics.readthedocs.io/en/latest/index.html) to compute metrics. TorchMetrics is a collection of metrics for PyTorch. It allows us to avoid writing boilerplate code for computing metrics like accuracy, precision, recall, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class CNNLit(pl.LightningModule):\n",
    "    \"\"\"Convolutional neural network model in PyTorch Lightning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        num_channels: int,\n",
    "        num_classes: int,\n",
    "        optimizer_args: Dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor method for CNNLit.\n",
    "\n",
    "        Args:\n",
    "            num_layers: the number of layers\n",
    "            num_channels: the number of channels\n",
    "            num_classes: the number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = 1 if i == 0 else num_channels\n",
    "            out_channels = num_channels if i < num_layers - 1 else num_classes\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.optimizer_args = optimizer_args\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize the parameters.\n",
    "\n",
    "        The weight is initialized using Xavier uniform initialization and the bias is initialized to zero.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "\n",
    "        Returns:\n",
    "            the logits\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            the optimizer\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.parameters(), **self.optimizer_args)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "\n",
    "        Returns:\n",
    "            the loss\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Validation step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Test step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\"test_acc\", acc, prog_bar=True, on_step=False, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | layers    | ModuleList         | 12.5 K\n",
      "1 | criterion | CrossEntropyLoss   | 0     \n",
      "2 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca12d08f38034cceb92c4b497e80cd5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52097062c21f4b81a1af610f83351b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e17c86faa14847b532979327e952ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41663467078743b38bda972792dd1734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8240000009536743     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.604171872138977     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8240000009536743    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.604171872138977    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.604171872138977, 'test_acc': 0.8240000009536743}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(SEED)  # reproducibility\n",
    "\n",
    "model = CNNLit(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    optimizer_args={\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"momentum\": MOMENTUM,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    ")\n",
    "datamodule = MNISTDataModule(\n",
    "    data_dir=\"data\", batch_size=BATCH_SIZE, train_size=50000, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing metrics\n",
    "\n",
    "We can visualize the metrics logged by Lightning using [TensorBoard](https://www.tensorflow.org/tensorboard) or [Weights & Biases](https://wandb.ai/site). We will be using TensorBoard in this tutorial. To install TensorBoard, you can use `pip install tensorboard`. To start TensorBoard, you can use the following command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir lightning_logs/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class CNNLit(pl.LightningModule):\n",
    "    \"\"\"Convolutional neural network model in PyTorch Lightning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int,\n",
    "        num_channels: int,\n",
    "        num_classes: int,\n",
    "        optimizer_args: Dict[str, Any],\n",
    "    ) -> None:\n",
    "        \"\"\"Constructor method for CNNLit.\n",
    "\n",
    "        Args:\n",
    "            num_layers: the number of layers\n",
    "            num_channels: the number of channels\n",
    "            num_classes: the number of classes\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            in_channels = 1 if i == 0 else num_channels\n",
    "            out_channels = num_channels if i < num_layers - 1 else num_classes\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    kernel_size=3,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                )\n",
    "            )\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.optimizer_args = optimizer_args\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        \"\"\"Initialize the parameters.\n",
    "\n",
    "        The weight is initialized using Xavier uniform initialization and the bias is initialized to zero.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(layer.weight)\n",
    "                nn.init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: the input tensor\n",
    "\n",
    "        Returns:\n",
    "            the logits\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "    def configure_optimizers(self) -> torch.optim.Optimizer:\n",
    "        \"\"\"Configure the optimizer.\n",
    "\n",
    "        Returns:\n",
    "            the optimizer\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.SGD(self.parameters(), **self.optimizer_args)\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Training step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "\n",
    "        Returns:\n",
    "            the loss\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        self.log(\n",
    "            \"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Validation step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.log(\n",
    "            \"val_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, logger=True\n",
    "        )\n",
    "\n",
    "    def test_step(\n",
    "        self, batch: Tuple[torch.Tensor, torch.Tensor], batch_idx: int\n",
    "    ) -> None:\n",
    "        \"\"\"Test step.\n",
    "\n",
    "        This method is called for every batch.\n",
    "\n",
    "        Args:\n",
    "            batch: the batch\n",
    "            batch_idx: the batch index\n",
    "        \"\"\"\n",
    "        x_batch, y_batch = batch\n",
    "        pred = self(x_batch)\n",
    "        loss = self.criterion(pred, y_batch)\n",
    "        acc = self.accuracy(pred, y_batch)\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "        self.log(\n",
    "            \"test_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, logger=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: lightning_logs/mnist\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | layers    | ModuleList         | 12.5 K\n",
      "1 | criterion | CrossEntropyLoss   | 0     \n",
      "2 | accuracy  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "12.5 K    Trainable params\n",
      "0         Non-trainable params\n",
      "12.5 K    Total params\n",
      "0.050     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e866a873024199a377ae6a2a6411bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1391fcb7784ee98187791434fb8968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428797c8c5b04d998befcbb07dfede51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd5a2c81f324abc892aa898688d36ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392c75d7e7de481fb0ae29c89e77f0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c959885f1fbe4ae487ee54f52e1451f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e06daf77d3d45ccb0083ea7b6e57732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da486b4cf2941fe8f38c280f798e2a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961b890fb32e420eb59422061ae8c3f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa1b764207940e18545e587e1e1490b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d828ce8b044df8bfff702eb32270c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9a933ede1e45368286c0a4419db79c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ea42133623940999e9e206633117521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f4d98afc0814c79aac23e455459a1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12eda3989b3845c8b8adec03e13b8d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92470b806c9c4fb58a9d747b0976d101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05efe03f64954dfb83c53647ba832906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27e9c5d7b6d5475a8e36feebdad90c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e0d9ff1866c417e875b5961270356e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "642112a00dec4b8a966a81607fce9076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b540f7b5603840fa804f61fbc2c3d026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc845be455d740a6979a2b5fabfd1b67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508b9cb347a846b9abfbf02d95f56a11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9617999792098999     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12229259312152863    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9617999792098999    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12229259312152863   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.12229259312152863, 'test_acc': 0.9617999792098999}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(SEED)  # reproducibility\n",
    "\n",
    "model = CNNLit(\n",
    "    num_layers=NUM_LAYERS,\n",
    "    num_channels=NUM_CHANNELS,\n",
    "    num_classes=NUM_CLASSES,\n",
    "    optimizer_args={\n",
    "        \"lr\": LEARNING_RATE,\n",
    "        \"momentum\": MOMENTUM,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "    },\n",
    ")\n",
    "datamodule = MNISTDataModule(\n",
    "    data_dir=\"data\", batch_size=BATCH_SIZE, train_size=50000, num_workers=NUM_WORKERS\n",
    ")\n",
    "\n",
    "logger = pl.loggers.TensorBoardLogger(\"lightning_logs/\", name=\"mnist\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else None,\n",
    "    logger=logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule)\n",
    "trainer.test(model, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing\n",
    "\n",
    "Lightning automatically saves checkpoints of your model at the end of every epoch. If training is interrupted, you can resume from the last saved checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865880c552f41099538e47a961aba9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9617999792098999     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.12229259312152863    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9617999792098999    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.12229259312152863   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.12229259312152863, 'test_acc': 0.9617999792098999}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the best model\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "ckpt = torch.load(best_model_path)\n",
    "model.load_state_dict(ckpt[\"state_dict\"])\n",
    "\n",
    "trainer.test(model, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_gsl_3.10.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
